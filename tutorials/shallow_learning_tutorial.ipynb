{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using shallow learning algorithms from ManufacturingNet\n",
    "##### To know more about ManufacturingNet please visit: http://manufacturingnet.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')  # add parent directory to path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ManufacturingNet\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import manufacturingnet. We can use this to experiment with several shallow learning models.\n",
    "\n",
    "It is important to note that all the dependencies of the package must also be installed in your environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now the dataset first needs to be downloaded. The dataset class can be used where different types of datasets have been curated and only two lines of code need to be run to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ManufacturingNet import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.ThreeDPrintingData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright! Now the dataset desired should be downloaded and present in the working directory.\n",
    "\n",
    "The 3D Printing dataset consists of several continuous and discrete parameters. We can perform classification or regression depending on what the desired output attribute is. We perform classification by predicting the material used based on the input and measured parameters. We can then perform regression on possibly a different attribute in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset\n",
    "Here, we can use the pandas library to read and import the data, since there are categorial attributes. If pandas is not installed in your environment, here is a useful reference : https://pandas.pydata.org/docs/getting_started/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"3D_printing_dataset/data.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then discretize the categorical attributes - infill pattern and material. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.material = [0 if each == \"abs\" else 1 for each in data.material]\n",
    "# abs = 0, pla = 1\n",
    "\n",
    "data.infill_pattern = [0 if each == \"grid\" else 1 for each in data.infill_pattern]\n",
    "# grid = 0, honeycomb = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification, we need the input data and an output variable to be predicted. \n",
    "We then separate our x and y values from the pandas dataframe. The value we want to predict is the \"material\", and our input data will be all the columns except \"material\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = data.material.values\n",
    "x_data = data.drop([\"material\"],axis=1).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get a birds-eye view of how the data can perform with some default classifiers. The metrics we use to measure the performance of these classifiers with some default values are Accuracy, 5-Fold cross validation, and the time. \n",
    "\n",
    "This will allow users to get a glance of how possible classifiers can perform on their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ManufacturingNet.models import AllClassificationModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "= All Classifier Models Parameter Inputs =\n",
      "==========================================\n",
      "verbose = True\n",
      "test_size = 0.2\n",
      "\n",
      "===========================================\n",
      "= End of inputs; press enter to continue. =\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/home/akshay/miniconda3/envs/basys/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/home/akshay/miniconda3/envs/basys/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/home/akshay/miniconda3/envs/basys/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/home/akshay/miniconda3/envs/basys/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/home/akshay/miniconda3/envs/basys/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      " This problem is unconstrained.\n",
      "/home/akshay/miniconda3/envs/basys/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           12     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+01    |proj g|=  4.12500D+02\n",
      "\n",
      "At iterate   50    f=  1.51161D+00    |proj g|=  3.35865D-02\n",
      "\n",
      "At iterate  100    f=  1.50955D+00    |proj g|=  1.43180D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   12    100    116      1     0     0   1.432D-01   1.510D+00\n",
      "  F =   1.5095472175421287     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           12     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+01    |proj g|=  5.38000D+02\n",
      "\n",
      "At iterate   50    f=  1.54852D+00    |proj g|=  5.63329D+00\n",
      "\n",
      "At iterate  100    f=  1.53926D+00    |proj g|=  4.24037D-02\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   12    100    122      1     0     0   4.240D-02   1.539D+00\n",
      "  F =   1.5392620453684027     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           12     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+01    |proj g|=  4.81500D+02\n",
      "\n",
      "At iterate   50    f=  1.53952D+00    |proj g|=  7.73077D-01\n",
      "\n",
      "At iterate  100    f=  1.52741D+00    |proj g|=  3.08703D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   12    100    113      1     0     0   3.087D-01   1.527D+00\n",
      "  F =   1.5274085955818686     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           12     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+01    |proj g|=  3.38500D+02\n",
      "\n",
      "At iterate   50    f=  1.52932D+00    |proj g|=  8.26908D-01\n",
      "\n",
      "At iterate  100    f=  1.51960D+00    |proj g|=  1.00590D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   12    100    114      1     0     0   1.006D-01   1.520D+00\n",
      "  F =   1.5196035878609195     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           12     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+01    |proj g|=  4.44000D+02\n",
      "\n",
      "At iterate   50    f=  1.53673D+00    |proj g|=  8.64991D-01\n",
      "\n",
      "At iterate  100    f=  1.51386D+00    |proj g|=  3.74503D-01\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   12    100    118      1     0     0   3.745D-01   1.514D+00\n",
      "  F =   1.5138605528682629     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n",
      "RUNNING THE L-BFGS-B CODE\n",
      "\n",
      "           * * *\n",
      "\n",
      "Machine precision = 2.220D-16\n",
      " N =           12     M =           10\n",
      "\n",
      "At X0         0 variables are exactly at the bounds\n",
      "\n",
      "At iterate    0    f=  2.77259D+01    |proj g|=  4.84000D+02\n",
      "\n",
      "At iterate   50    f=  1.46892D+00    |proj g|=  1.72575D-01\n",
      "\n",
      "At iterate  100    f=  1.46141D+00    |proj g|=  1.24604D+00\n",
      "\n",
      "           * * *\n",
      "\n",
      "Tit   = total number of iterations\n",
      "Tnf   = total number of function evaluations\n",
      "Tnint = total number of segments explored during Cauchy searches\n",
      "Skip  = number of BFGS updates skipped\n",
      "Nact  = number of active bounds at final generalized Cauchy point\n",
      "Projg = norm of the final projected gradient\n",
      "F     = final function value\n",
      "\n",
      "           * * *\n",
      "\n",
      "   N    Tit     Tnf  Tnint  Skip  Nact     Projg        F\n",
      "   12    100    114      1     0     0   1.246D+00   1.461D+00\n",
      "  F =   1.4614140826778006     \n",
      "\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT                 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "/home/akshay/miniconda3/envs/basys/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/akshay/miniconda3/envs/basys/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/akshay/miniconda3/envs/basys/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/akshay/miniconda3/envs/basys/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/akshay/miniconda3/envs/basys/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/akshay/miniconda3/envs/basys/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]*\n",
      "optimization finished, #iter = 23\n",
      "obj = -22.939326, rho = -0.163843\n",
      "nSV = 27, nBSV = 23\n",
      "Total nSV = 27\n",
      "*\n",
      "optimization finished, #iter = 24\n",
      "obj = -25.153360, rho = 0.159480\n",
      "nSV = 30, nBSV = 27\n",
      "Total nSV = 30\n",
      "*.*\n",
      "optimization finished, #iter = 36\n",
      "obj = -20.443385, rho = -0.181031\n",
      "nSV = 26, nBSV = 21\n",
      "Total nSV = 26\n",
      "*\n",
      "optimization finished, #iter = 24\n",
      "obj = -23.232356, rho = 0.150667\n",
      "nSV = 28, nBSV = 23\n",
      "Total nSV = 28\n",
      "*.*\n",
      "optimization finished, #iter = 41\n",
      "obj = -22.459159, rho = -0.491829\n",
      "nSV = 26, nBSV = 20\n",
      "Total nSV = 26\n",
      "*.*\n",
      "optimization finished, #iter = 55\n",
      "obj = -28.355148, rho = 0.059459\n",
      "nSV = 35, nBSV = 29\n",
      "Total nSV = 35\n",
      "[LibSVM]*\n",
      "optimization finished, #iter = 16\n",
      "obj = -24.325072, rho = -0.051267\n",
      "nSV = 29, nBSV = 26\n",
      "Total nSV = 29\n",
      "*\n",
      "optimization finished, #iter = 23\n",
      "obj = -26.160815, rho = 0.974156\n",
      "nSV = 30, nBSV = 26\n",
      "Total nSV = 30\n",
      "*\n",
      "optimization finished, #iter = 18\n",
      "obj = -22.805646, rho = 0.112783\n",
      "nSV = 27, nBSV = 24\n",
      "Total nSV = 27\n",
      "*\n",
      "optimization finished, #iter = 22\n",
      "obj = -23.312385, rho = -0.149942\n",
      "nSV = 27, nBSV = 24\n",
      "Total nSV = 27\n",
      "*\n",
      "optimization finished, #iter = 16\n",
      "obj = -25.540438, rho = 0.105540\n",
      "nSV = 30, nBSV = 28\n",
      "Total nSV = 30\n",
      "*\n",
      "optimization finished, #iter = 22\n",
      "obj = -30.844117, rho = -0.091197\n",
      "nSV = 36, nBSV = 33\n",
      "Total nSV = 36\n",
      "[LibSVM]*.*\n",
      "optimization finished, #iter = 35\n",
      "obj = -22.052330, rho = 0.194729\n",
      "nSV = 28, nBSV = 22\n",
      "Total nSV = 28\n",
      "*\n",
      "optimization finished, #iter = 16\n",
      "obj = -26.280381, rho = 0.177320\n",
      "nSV = 30, nBSV = 28\n",
      "Total nSV = 30\n",
      "*\n",
      "optimization finished, #iter = 17\n",
      "obj = -25.005912, rho = 0.310278\n",
      "nSV = 30, nBSV = 27\n",
      "Total nSV = 30\n",
      "*\n",
      "optimization finished, #iter = 23\n",
      "obj = -25.864392, rho = 0.708984\n",
      "nSV = 30, nBSV = 26\n",
      "Total nSV = 30\n",
      "*\n",
      "optimization finished, #iter = 31\n",
      "obj = -24.567575, rho = 0.047160\n",
      "nSV = 29, nBSV = 25\n",
      "Total nSV = 29\n",
      "*\n",
      "optimization finished, #iter = 29\n",
      "obj = -31.111441, rho = -0.137588\n",
      "nSV = 36, nBSV = 32\n",
      "Total nSV = 36\n",
      "[LibSVM]*\n",
      "optimization finished, #iter = 17\n",
      "obj = -27.748990, rho = -0.487415\n",
      "nSV = 31, nBSV = 29\n",
      "Total nSV = 31\n",
      "*\n",
      "optimization finished, #iter = 22\n",
      "obj = -26.081164, rho = -0.016539\n",
      "nSV = 31, nBSV = 28\n",
      "Total nSV = 31\n",
      "*\n",
      "optimization finished, #iter = 18\n",
      "obj = -26.128584, rho = 0.014025\n",
      "nSV = 30, nBSV = 28\n",
      "Total nSV = 30\n",
      "*\n",
      "optimization finished, #iter = 21\n",
      "obj = -25.787701, rho = 0.206712\n",
      "nSV = 30, nBSV = 28\n",
      "Total nSV = 30\n",
      "*\n",
      "optimization finished, #iter = 19\n",
      "obj = -27.537953, rho = 0.719423\n",
      "nSV = 32, nBSV = 29\n",
      "Total nSV = 32\n",
      "*\n",
      "optimization finished, #iter = 30\n",
      "obj = -33.445042, rho = -0.099424\n",
      "nSV = 39, nBSV = 35\n",
      "Total nSV = 39\n",
      "[LibSVM]*\n",
      "optimization finished, #iter = 21\n",
      "obj = -24.953563, rho = 0.064917\n",
      "nSV = 29, nBSV = 27\n",
      "Total nSV = 29\n",
      "*\n",
      "optimization finished, #iter = 17\n",
      "obj = -25.360165, rho = -0.070278\n",
      "nSV = 29, nBSV = 26\n",
      "Total nSV = 29\n",
      "*\n",
      "optimization finished, #iter = 21\n",
      "obj = -26.356307, rho = 0.101884\n",
      "nSV = 31, nBSV = 27\n",
      "Total nSV = 31\n",
      "*\n",
      "optimization finished, #iter = 16\n",
      "obj = -27.056138, rho = 0.370646\n",
      "nSV = 31, nBSV = 29\n",
      "Total nSV = 31\n",
      "*\n",
      "optimization finished, #iter = 15\n",
      "obj = -26.371519, rho = 0.808824\n",
      "nSV = 29, nBSV = 27\n",
      "Total nSV = 29\n",
      "*.*\n",
      "optimization finished, #iter = 40\n",
      "obj = -32.444579, rho = -0.017345\n",
      "nSV = 38, nBSV = 35\n",
      "Total nSV = 38\n",
      "[LibSVM]*.*\n",
      "optimization finished, #iter = 32\n",
      "obj = -23.472622, rho = -0.046408\n",
      "nSV = 29, nBSV = 24\n",
      "Total nSV = 29\n",
      "*\n",
      "optimization finished, #iter = 19\n",
      "obj = -25.208652, rho = 0.149472\n",
      "nSV = 30, nBSV = 27\n",
      "Total nSV = 30\n",
      "*.*\n",
      "optimization finished, #iter = 34\n",
      "obj = -22.496039, rho = 0.022836\n",
      "nSV = 29, nBSV = 23\n",
      "Total nSV = 29\n",
      "*\n",
      "optimization finished, #iter = 16\n",
      "obj = -26.900139, rho = 0.817072\n",
      "nSV = 31, nBSV = 29\n",
      "Total nSV = 31\n",
      "*\n",
      "optimization finished, #iter = 15\n",
      "obj = -26.367059, rho = 0.221648\n",
      "nSV = 30, nBSV = 30\n",
      "Total nSV = 30\n",
      "*\n",
      "optimization finished, #iter = 30\n",
      "obj = -30.861981, rho = -0.121902\n",
      "nSV = 36, nBSV = 33\n",
      "Total nSV = 36\n",
      "[LibSVM]...*\n",
      "optimization finished, #iter = 117\n",
      "C = 16.553796\n",
      "obj = 101.809478, rho = -0.671898\n",
      "nSV = 24, nBSV = 11\n",
      "Total nSV = 24\n",
      "....*\n",
      "optimization finished, #iter = 129\n",
      "C = 22.249582\n",
      "obj = 159.257197, rho = -0.003434\n",
      "nSV = 22, nBSV = 9\n",
      "Total nSV = 22\n",
      "...*\n",
      "optimization finished, #iter = 117\n",
      "C = 11.633120\n",
      "obj = 63.140148, rho = -0.253406\n",
      "nSV = 23, nBSV = 12\n",
      "Total nSV = 23\n",
      "...*\n",
      "optimization finished, #iter = 126\n",
      "C = 20.749922\n",
      "obj = 145.450912, rho = 0.186688\n",
      "nSV = 21, nBSV = 10\n",
      "Total nSV = 21\n",
      ".*\n",
      "optimization finished, #iter = 58\n",
      "C = 11.159379\n",
      "obj = 56.263211, rho = 0.225533\n",
      "nSV = 22, nBSV = 13\n",
      "Total nSV = 22\n",
      ".\n",
      "Warning: using -h 0 may be faster\n",
      ".*\n",
      "optimization finished, #iter = 102\n",
      "C = 15.023741\n",
      "obj = 121.478904, rho = -0.043133\n",
      "nSV = 25, nBSV = 14\n",
      "Total nSV = 25\n",
      "[LibSVM]..*\n",
      "optimization finished, #iter = 73\n",
      "C = 16.643153\n",
      "obj = 100.418890, rho = 0.927208\n",
      "nSV = 22, nBSV = 13\n",
      "Total nSV = 22\n",
      "..*\n",
      "optimization finished, #iter = 83\n",
      "C = 17.662414\n",
      "obj = 103.714951, rho = 0.597879\n",
      "nSV = 21, nBSV = 11\n",
      "Total nSV = 21\n",
      ".\n",
      "Warning: using -h 0 may be faster\n",
      ".*\n",
      "optimization finished, #iter = 87\n",
      "C = 24.382135\n",
      "obj = 179.789281, rho = -0.023566\n",
      "nSV = 20, nBSV = 11\n",
      "Total nSV = 20\n",
      "..*\n",
      "optimization finished, #iter = 64\n",
      "C = 25.077262\n",
      "obj = 189.597368, rho = -0.120324\n",
      "nSV = 21, nBSV = 10\n",
      "Total nSV = 21\n",
      "...*\n",
      "optimization finished, #iter = 121\n",
      "C = 20.197361\n",
      "obj = 135.797251, rho = -0.018672\n",
      "nSV = 23, nBSV = 10\n",
      "Total nSV = 23\n",
      ".\n",
      "Warning: using -h 0 may be faster\n",
      ".*\n",
      "optimization finished, #iter = 86\n",
      "C = 19.543412\n",
      "obj = 179.812610, rho = 0.021996\n",
      "nSV = 24, nBSV = 14\n",
      "Total nSV = 24\n",
      "[LibSVM].*..*\n",
      "optimization finished, #iter = 96\n",
      "C = 23.060922\n",
      "obj = 164.544256, rho = -0.486290\n",
      "nSV = 22, nBSV = 11\n",
      "Total nSV = 22\n",
      "...*\n",
      "optimization finished, #iter = 111\n",
      "C = 23.084905\n",
      "obj = 159.347844, rho = -0.578551\n",
      "nSV = 20, nBSV = 12\n",
      "Total nSV = 20\n",
      "......*\n",
      "optimization finished, #iter = 193\n",
      "C = 20.699498\n",
      "obj = 138.937110, rho = -0.289171\n",
      "nSV = 24, nBSV = 10\n",
      "Total nSV = 24\n",
      "...*\n",
      "optimization finished, #iter = 115\n",
      "C = 13.215135\n",
      "obj = 69.101822, rho = -0.031559\n",
      "nSV = 21, nBSV = 11\n",
      "Total nSV = 21\n",
      "...*\n",
      "optimization finished, #iter = 111\n",
      "C = 20.907711\n",
      "obj = 139.671653, rho = -0.396108\n",
      "nSV = 21, nBSV = 11\n",
      "Total nSV = 21\n",
      "....*\n",
      "optimization finished, #iter = 196\n",
      "C = 20.442521\n",
      "obj = 193.180653, rho = 0.353129\n",
      "nSV = 24, nBSV = 13\n",
      "Total nSV = 24\n",
      "[LibSVM]......*\n",
      "optimization finished, #iter = 197\n",
      "C = 22.444407\n",
      "obj = 164.438018, rho = -0.584466\n",
      "nSV = 22, nBSV = 11\n",
      "Total nSV = 22\n",
      "...*\n",
      "optimization finished, #iter = 109\n",
      "C = 18.504041\n",
      "obj = 121.580675, rho = -0.098530\n",
      "nSV = 25, nBSV = 10\n",
      "Total nSV = 25\n",
      ".\n",
      "Warning: using -h 0 may be faster\n",
      "..*\n",
      "optimization finished, #iter = 108\n",
      "C = 22.952538\n",
      "obj = 169.396074, rho = -0.196399\n",
      "nSV = 21, nBSV = 9\n",
      "Total nSV = 21\n",
      "...*\n",
      "optimization finished, #iter = 117\n",
      "C = 18.449740\n",
      "obj = 121.587575, rho = -0.110544\n",
      "nSV = 21, nBSV = 11\n",
      "Total nSV = 21\n",
      "......*..*\n",
      "optimization finished, #iter = 230\n",
      "C = 23.145606\n",
      "obj = 174.085201, rho = -0.702193\n",
      "nSV = 24, nBSV = 9\n",
      "Total nSV = 24\n",
      ".....*\n",
      "optimization finished, #iter = 229\n",
      "C = 18.619688\n",
      "obj = 171.593270, rho = 0.552318\n",
      "nSV = 27, nBSV = 15\n",
      "Total nSV = 27\n",
      "[LibSVM]....*\n",
      "optimization finished, #iter = 149\n",
      "C = 20.641622\n",
      "obj = 142.732616, rho = -0.622757\n",
      "nSV = 23, nBSV = 9\n",
      "Total nSV = 23\n",
      "....*\n",
      "optimization finished, #iter = 146\n",
      "C = 16.560036\n",
      "obj = 108.520149, rho = -0.451563\n",
      "nSV = 21, nBSV = 10\n",
      "Total nSV = 21\n",
      "...*\n",
      "optimization finished, #iter = 104\n",
      "C = 18.672736\n",
      "obj = 120.632888, rho = -0.730022\n",
      "nSV = 23, nBSV = 11\n",
      "Total nSV = 23\n",
      "....*\n",
      "optimization finished, #iter = 140\n",
      "C = 22.131133\n",
      "obj = 154.380863, rho = -0.918013\n",
      "nSV = 23, nBSV = 10\n",
      "Total nSV = 23\n",
      ".....*\n",
      "optimization finished, #iter = 185\n",
      "C = 21.390775\n",
      "obj = 149.813697, rho = 0.212239\n",
      "nSV = 21, nBSV = 10\n",
      "Total nSV = 21\n",
      ".....*\n",
      "optimization finished, #iter = 221\n",
      "C = 18.492918\n",
      "obj = 164.190344, rho = 0.716647\n",
      "nSV = 25, nBSV = 14\n",
      "Total nSV = 25\n",
      "[LibSVM]....*\n",
      "optimization finished, #iter = 141\n",
      "C = 17.573508\n",
      "obj = 128.987794, rho = 0.008166\n",
      "nSV = 20, nBSV = 8\n",
      "Total nSV = 20\n",
      "...*\n",
      "optimization finished, #iter = 114\n",
      "C = 18.864887\n",
      "obj = 137.264587, rho = -0.232450\n",
      "nSV = 21, nBSV = 12\n",
      "Total nSV = 21\n",
      ".\n",
      "Warning: using -h 0 may be faster\n",
      "..*\n",
      "optimization finished, #iter = 120\n",
      "C = 18.556222\n",
      "obj = 137.445080, rho = -0.101976\n",
      "nSV = 21, nBSV = 11\n",
      "Total nSV = 21\n",
      ".*\n",
      "optimization finished, #iter = 43\n",
      "C = 15.566738\n",
      "obj = 107.875697, rho = -0.597339\n",
      "nSV = 19, nBSV = 10\n",
      "Total nSV = 19\n",
      "...*\n",
      "optimization finished, #iter = 124\n",
      "C = 15.007024\n",
      "obj = 98.111741, rho = -0.121086\n",
      "nSV = 20, nBSV = 11\n",
      "Total nSV = 20\n",
      "..*\n",
      "optimization finished, #iter = 107\n",
      "C = 15.060112\n",
      "obj = 136.583203, rho = 0.349540\n",
      "nSV = 24, nBSV = 15\n",
      "Total nSV = 24\n",
      "[LibLinear]....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -0.061361\n",
      "nSV = 33\n",
      "[LibLinear]....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -0.061253\n",
      "[LibLinear]nSV = 35\n",
      "....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -0.063584\n",
      "nSV = 31\n",
      "[LibLinear]....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "[LibLinear]Objective value = -0.063677\n",
      "nSV = 31\n",
      "....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -0.062368\n",
      "nSV = 31\n",
      "[LibLinear]....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 2 may be faster (also see FAQ)\n",
      "\n",
      "Objective value = -0.063430\n",
      "nSV = 26\n",
      "\n",
      "===========\n",
      "= Results =\n",
      "===========\n",
      "\n",
      "Model                Accuracy             5-Fold CV Mean       Time (seconds)      \n",
      "\n",
      "LogisticRegression   1.0                  1.0                  0.03149294853210449 \n",
      "\n",
      "RandomForest         0.9                  0.9800000000000001   0.17609453201293945 \n",
      "\n",
      "SVC                  0.3                  0.6                  0.0017521381378173828\n",
      "\n",
      "NuSVC                1.0                  0.9800000000000001   0.0018279552459716797\n",
      "\n",
      "LinearSVC            1.0                  1.0                  0.0037686824798583984\n",
      "\n",
      "XGBClassifier        1.0                  1.0                  1.2896432876586914  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_models = AllClassificationModels(x_data, y_data)\n",
    "all_models.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user wants to modify a particular classifier more specifically, they are free to choose the classifier they want and pass the data to that.\n",
    "\n",
    "The user can either choose to persist with the default parameters displayed or can customize the parameters according to their requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===========================================\n",
      "= RandomForestClassifier Parameter Inputs =\n",
      "===========================================\n",
      "\n",
      "Default values:\n",
      "test_size = 0.25\n",
      "cv = 5\n",
      "graph_results = False\n",
      "criterion = 'gini'\n",
      "class_weight = None\n",
      "n_estimators = 100\n",
      "max_depth = None\n",
      "min_samples_split = 2\n",
      "min_samples_leaf = 1\n",
      "min_weight_fraction_leaf = 0.0\n",
      "max_features = 'auto'\n",
      "max_leaf_nodes = None\n",
      "min_impurity_decrease = 0.0\n",
      "bootstrap = True\n",
      "oob_score = False\n",
      "n_jobs = None\n",
      "random_state = None\n",
      "verbose = 0\n",
      "warm_start = False\n",
      "ccp_alpha = 0.0\n",
      "max_samples = None\n",
      "\n",
      "===========================================\n",
      "= End of inputs; press enter to continue. =\n",
      "\n",
      "==================================\n",
      "= RandomForestClassifier Results =\n",
      "==================================\n",
      "\n",
      "Classes:\n",
      " [0 1]\n",
      "\n",
      "Accuracy:            1.0                 \n",
      "\n",
      "ROC AUC:             1.0                 \n",
      "\n",
      "Cross Validation Scores: [1.  1.  1.  0.8 1. ]\n",
      "\n",
      "Feature Importances: [0.03085996 0.0673289  0.08198325 0.02094606 0.38845729 0.05159277\n",
      " 0.00707522 0.06071114 0.0664804  0.10127104 0.12329398]\n",
      "\n",
      "\n",
      "Call predict_classifier() to make predictions for new data.\n",
      "\n",
      "===================\n",
      "= End of results. =\n",
      "===================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ManufacturingNet.models import RandomForest\n",
    "\n",
    "rf_model = RandomForest(x_data, y_data)\n",
    "\n",
    "rf_model.run_classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regression, we need the input data and an output value to be obtained. \n",
    "We then separate our x and y values from the pandas dataframe. In this example, the value we want to output is the \"roughness\", and our input data will be all the columns except \"roughness\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data_lin = data.roughness.values\n",
    "x_data_lin = data.drop([\"roughness\"],axis=1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first get a birds-eye view of how the data can perform with some default regression models. The metrics we use to measure the performance of these regression models with some default parameters are R-2 score and the time taken to run the algorithm. \n",
    "\n",
    "This will allow users to get a glance of how possible regression models can perform on their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==========================================\n",
      "= All Regression Models Parameter Inputs =\n",
      "==========================================\n",
      "\n",
      "verbose = True\n",
      "test_size = 0.2\n",
      "\n",
      "===========================================\n",
      "= End of inputs; press enter to continue. =\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "/home/akshay/miniconda3/envs/basys/lib/python3.8/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM][LibSVM]*\n",
      "optimization finished, #iter = 20\n",
      "obj = -2938.211052, rho = -165.272761\n",
      "nSV = 40, nBSV = 40\n",
      "*\n",
      "optimization finished, #iter = 18\n",
      "epsilon = 65.172899\n",
      "obj = -2345.500603, rho = -157.239148\n",
      "nSV = 20, nBSV = 20\n",
      "[LibLinear]....................................................................................................\n",
      "optimization finished, #iter = 1000\n",
      "\n",
      "WARNING: reaching max number of iterations\n",
      "Using -s 11 may be faster\n",
      "\n",
      "Objective value = -2249.829485\n",
      "nSV = 40\n",
      "\n",
      "===========\n",
      "= Results =\n",
      "===========\n",
      "\n",
      "Model                R2 Score             Time (seconds)      \n",
      "\n",
      "LinearRegression     0.8495032715191071   0.011422872543334961\n",
      "\n",
      "RandomForest         0.790419536435486    0.12665534019470215 \n",
      "\n",
      "SVR                  -0.0024695127050318177 0.0007905960083007812\n",
      "\n",
      "NuSVR                -0.010896841754949094 0.0004944801330566406\n",
      "\n",
      "LinearSVR            -0.5892127676794754  0.001963376998901367\n",
      "\n",
      "XGBRegressor         0.9062022279834505   2.391239881515503   \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ManufacturingNet.models import AllRegressionModels\n",
    "\n",
    "models_reg = AllRegressionModels(x_data_lin, y_data_lin)\n",
    "models_reg.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user wants to modify a particular regression model more specifically, they are free to choose the model they want and pass the data to that.\n",
    "\n",
    "The user can either choose to persist with the default parameters displayed or can customize the parameters according to their requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================\n",
      "= LinRegression Parameter Inputs =\n",
      "==================================\n",
      "\n",
      "Default values:\n",
      "test_size = 0.25\n",
      "cv = 5\n",
      "graph_results = False\n",
      "fit_intercept = True\n",
      "normalize = False\n",
      "copy_X = True\n",
      "n_jobs = None\n",
      "\n",
      "===========================================\n",
      "= End of inputs; press enter to continue. =\n",
      "\n",
      "=========================\n",
      "= LinRegression Results =\n",
      "=========================\n",
      "\n",
      "Coefficients:\n",
      " [ 1.39931120e+03  1.84748087e+00  1.06309991e-01 -1.12894328e+01\n",
      "  1.26434657e+01 -5.52559352e-01  6.36071840e-01  2.64259648e+02\n",
      " -2.76279676e+00  1.17877621e+00 -3.52518138e+01]\n",
      "\n",
      "Intercept:           -2747.1297188706626 \n",
      "\n",
      "Mean Squared Error:  1496.9165446449076  \n",
      "\n",
      "R2 Score:            0.8965442504291975  \n",
      "\n",
      "R Score:             0.9468602063817011  \n",
      "\n",
      "Cross Validation Scores:\n",
      " [-6.83994071  0.28585519  0.48941658  0.3642745  -0.75248048]\n",
      "\n",
      "\n",
      "Call predict() to make predictions for new data.\n",
      "\n",
      "===================\n",
      "= End of results. =\n",
      "===================\n",
      "\n",
      "MSE: 1496.9165446449076\n",
      "R2: 0.8965442504291975\n",
      "R: 0.9468602063817011\n"
     ]
    }
   ],
   "source": [
    "from ManufacturingNet.models import LinRegression as LinReg\n",
    "\n",
    "\n",
    "model_lin = LinReg(x_data_lin, y_data_lin)\n",
    "model_lin.run()\n",
    "\n",
    "print(\"MSE:\", model_lin.get_mean_squared_error())\n",
    "print(\"R2:\", model_lin.get_r2_score())\n",
    "print(\"R:\", model_lin.get_r_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we can use ManufacturingNet to accomplish classification and regression tasks. \n",
    "We can first obtain a birds-eye view of the performance of all the models that can be used with our data. If we want to modify  a particular model specifically for our data, we can customize the parameters for the model of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
