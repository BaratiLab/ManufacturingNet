from sklearn.neural_network import MLPClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, precision_score, recall_score

import numpy as np  



class MLP: 
    
    """
    Wrapper class around scikit-learn's logistic regression functionality.

    Logistic Regression is a type of Generalized Linear Model (GLM) that uses a logistic function to model a binary 
    variable based on any kind of independent variables.
    """


    def __init__(self, attributes=None, labels=None, test_size=0.25, hidden_layer_sizes=(100, ), \
    		activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', \
    			learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, \
    				warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, \
    					beta_2=0.999, epsilon=1e-08, n_iter_no_change=10):
        

        """
        Initializes an MLP object.

        The following parameters are needed to create a logistic regression model:

            - attributes: a numpy array of the desired independent variables
            - labels: a numpy array of the desired dependent variables
            - test_size: the proportion of the dataset to be used for testing the model (defaults to 0.25);
            the proportion of the dataset to be used for training will be the complement of test_size
            - fit_intercept: specifies if a constant (a.k.a. bias or intercept) should be added to the decision function. (defaults to True)
            - intercept_scaling: used only when the solver ‘liblinear’ is used and self.fit_intercept is set to True. Here, x becomes [x, self.intercept_scaling], 
            i.e. a “synthetic” feature with constant value equal to intercept_scaling is appended to the instance vector.
            - class_weight: weights associated with classes (defaults to None). If set to 'balanced' then the values of y are used 
             to automatically adjust weights inversely proportional to class frequencies in the input data.
            - random_state: used when solver == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the data (defaults to None)
            - solver: chooses the algorithm for the optimization problem, (defaults to lbfgs)
            - max_iter: maximum number of iterations taken for the solvers to converge (defaults to 100)
            - multi_class: chooses if we fit a binary problem or a multi-class problem for each label (defaults to auto)
            - verbose: used in the liblinear or lbfgs solvers, where any positive number is set (defaults to 0)
            - warm_start: when set to true, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution (defaults to False)
            - n_jobs: the number of jobs to use for the computation (defaults to None)
            - l1_ratio: this is the Elastic-Net mixing parameter. Setting this to 0 is using l2 penalty, setting this to
            1 is using l1_penalty and a value between 0 and 1 is a combination of l1 and l2. 

        The following instance data is found after successfully running linear_regression():

            - coefficients: an array of coefficients that most closely satisfy the linear relationship between the
            independent and dependent variables
            - intercept: the y-intercept of the regression line generated by the model
            - classes: a list of class labels known to the classifier
            - n_iter : Actual number of iterations for all classes.
            - accuracy: the classification accuracy score
            - roc_auc: the area under the receiver operating characteristic curve from the prediction scores
        """

        self.attributes = attributes
        self.labels = labels
        self.test_size = test_size

        self.hidden_layer_sizes = hidden_layer_sizes
        self.activation = activation
        self.solver = solver
        self.alpha = alpha
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.learning_rate_init = learning_rate_init
        self.power_t = power_t
        self.max_iter = max_iter
        self.shuffle = shuffle
        self.random_state = random_state
        self.tol = tol
        self.verbose = verbose 
        self.warm_start = warm_start
        self.momentum = momentum
        self.nesterovs_momentum = nesterovs_momentum
        self.early_stopping = early_stopping
        self.validation_fraction = validation_fraction
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon=1e-08
        self.n_iter_no_change = n_iter_no_change

        self.classes_ = None
        self.coef_ = None
        self.loss = None
        self.intercept_ = None
        self.n_iter_ = None
        self.n_layers = None
        self.n_outputs = None
        self.out_activation = None

        self.accuracy = None
        self.roc_auc = None
        self.precision_scores = []
        self.precision = None
        self.recall_scores = []
        self.recall = None


    # Accessor methods

    def get_attributes(self):
        """
        Accessor method for attributes.

        If a MLP object is constructed without specifying attributes, attributes will be None.
        logistic_regression() cannot be called until attributes is a populated numpy array; call
        set_attributes(new_attributes) to fix this.
        """
        return self.attributes

    def get_labels(self):
        """
        Accessor method for labels.

        If a MLP object is constructed without specifying labels, labels will be None.
        logistic_regression() cannot be called until labels is a populated numpy array; call set_labels(new_labels)
        to fix this.
        """
        return self.labels

    
    def get_classes(self):
        """
        Accessor method for classes.

        Will return None if linear_regression() hasn't been called, yet.
        """
        return self.classes


    def get_coefficents(self):
        """
        Accessor method for coefficients.

        Will return None if linear_regression() hasn't been called, yet.
        """
        return self.coef_

    def get_n_iter_(self):
        """
        Accessor method for number of iterations for all classes.

        Will return None if linear_regression() hasn't been called, yet.
        """
        return self.n_iter_

    def get_accuracy(self):
        """
        Accessor method for accuracy.

        Will return None if linear_regression() hasn't been called, yet.
        """
        return self.accuracy

    def get_roc_auc(self):
        """
        Accessor method for roc-auc score.

        Will return None if linear_regression() hasn't been called, yet.
        """
        return self.roc_auc

    def get_precision(self, label):
        """
        Accessor method for precision.

        Will return None if linear_regression() hasn't been called, yet.
        """
        self.precision = self.precision_scores.get(label)
        return self.precision
    
    def get_recall(self, label):
        """
        Accessor method for precision.

        Will return None if linear_regression() hasn't been called, yet.
        """
        self.recall = self.recall_scores.get(label)
        return self.recall

    def set_attributes(self, new_attributes = None):
        """
        Modifier method for attributes.

        Input should be a populated numpy array of the desired independent variables.
        """
        self.attributes = new_attributes

    def set_labels(self, new_labels = None):
        """
        Modifier method for labels.

        Input should be a populated numpy array of the desired dependent variables.
        """
        self.labels = new_labels

    def set_test_size(self, new_test_size = None):
        """
        Modifier method for train_test_split.

        Input should be a list of strings, where each string is the name of a dependent variable.
        """
        self.test_size  = new_test_size


    # Wrapper for logistic regression model

    def MLP(self):
        """
        Performs logistic regression on dataset using scikit-learn's logistic_model and returns the resultant array of
        coefficients.
        """
        if self._check_inputs():
            # Instantiate MLP() object
            MLP = MLPClassifier(hidden_layer_sizes=self.hidden_layer_sizes, \
    			activation=self.activation, solver=self.solver, alpha=self.alpha, batch_size=self.batch_size, \
    				learning_rate=self.learning_rate, learning_rate_init=self.learning_rate_init, power_t=self.power_t, \
    					max_iter=self.max_iter, shuffle=self.shuffle, random_state=self.random_state, tol=self.tol, verbose=self.verbose, \
    						warm_start=self.warm_start, momentum=self.momentum, nesterovs_momentum=self.nesterovs_momentum, early_stopping=self.early_stopping,\
    							validation_fraction=self.validation_fraction, beta_1=self.beta_1, beta_2=self.beta_2, epsilon=self.epsilon, n_iter_no_change=self.n_iter_no_change)

            # Split into training and testing set
            dataset_X_train, dataset_X_test, dataset_y_train, dataset_y_test = \
                train_test_split(self.attributes,self.labels,test_size=self.test_size)

            # Train the model and get resultant coefficients
            MLP.fit(dataset_X_train, np.ravel(dataset_y_train))

            self.classes_ = MLP.classes_
            self.coefficients = MLP.coefs_
            self.n_iter_ = MLP.n_iter_
            self.loss = MLP.loss_
            self.n_layers_ = MLP.n_layers_
            self.n_outputs_ = MLP.n_outputs_
            self.out_activation_ = MLP.out_activation_


            # Make predictions using testing set
            y_prediction = MLP.predict(dataset_X_test)
            y_pred_probas = MLP.predict_proba(dataset_X_test)[::, 1]

            #Metrics
            self.accuracy = accuracy_score(y_prediction, dataset_y_test)
            self.roc_auc = roc_auc_score(y_prediction, y_pred_probas)

            self.precision_scores = { each : precision_score(dataset_y_test, y_prediction, pos_label=each) \
                                                                    for each in self.classes_}
            self.recall_scores = { each : recall_score(dataset_y_test, y_prediction, pos_label=each) \
                                                                    for each in self.classes_}


    # Helper method for checking inputs

    def _check_inputs(self):
        """
        Verifies if the instance data is ready for use in logistic regression model.
        """

        # Check if attributes exists
        if self.attributes is None or type(self.attributes) is not np.ndarray:
            print("attributes is missing; call set_attributes(new_attributes) to fix this! new_attributes should be a",
            "populated numpy array of your independent variables.")
            return False

        # Check if labels exists
        if self.labels is None or type(self.labels) is not np.ndarray:
            print("labels is missing; call set_labels(new_labels) to fix this! new_labels should be a populated numpy",
            "array of your dependent variables.")
            return False

        # Check if attributes and labels have same number of rows (samples)
        if self.attributes.shape[0] != self.labels.shape[0]:
            print("attributes and labels don't have the same number of rows. Make sure the number of samples in each",
                  "dataset matches!")
            return False

        # Check if test_size is a float or None
        if self.test_size is not None and not isinstance(self.test_size, (int, float)):
            print("test_size must be None or a number; call set_test_size(new_test_size) to fix this!")
            return False

        return True


